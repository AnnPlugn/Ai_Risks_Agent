# final_critical_fix.py
"""
–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç LangGraph –æ—à–∏–±–∫–∏ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSON –∞–≥–µ–Ω—Ç–∞–º–∏
"""

import sys
from pathlib import Path

def fix_langgraph_state_annotation():
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ LangGraph state annotation"""
    print("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ LangGraph state annotation...")
    
    risk_models_file = Path("src/models/risk_models.py")
    
    if not risk_models_file.exists():
        print("‚ùå –§–∞–π–ª risk_models.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    content = risk_models_file.read_text(encoding='utf-8')
    
    # –î–æ–±–∞–≤–ª—è–µ–º Annotated –∏–º–ø–æ—Ä—Ç –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if "from typing import Dict, List, Optional, Any, Union" in content:
        if "Annotated" not in content:
            content = content.replace(
                "from typing import Dict, List, Optional, Any, Union",
                "from typing import Dict, List, Optional, Any, Union, Annotated"
            )
            print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç Annotated")
    
    # –ò—â–µ–º WorkflowState –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è assessment_id
    if "class WorkflowState(BaseModel):" in content:
        # –ó–∞–º–µ–Ω—è–µ–º assessment_id –Ω–∞ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        old_assessment_id = 'assessment_id: Optional[str] = Field(None, description="ID –æ—Ü–µ–Ω–∫–∏")'
        
        # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π –¥–ª—è LangGraph
        new_assessment_id = '''assessment_id: Annotated[Optional[str], "assessment_id"] = Field(None, description="ID –æ—Ü–µ–Ω–∫–∏")'''
        
        if old_assessment_id in content:
            content = content.replace(old_assessment_id, new_assessment_id)
            print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è assessment_id")
        
        # –¢–∞–∫–∂–µ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ –ø–æ–ª—è –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        fields_to_annotate = [
            ('current_step: str = Field("initialization", description="–¢–µ–∫—É—â–∏–π —à–∞–≥")', 
             'current_step: Annotated[str, "current_step"] = Field("initialization", description="–¢–µ–∫—É—â–∏–π —à–∞–≥")'),
            ('evaluation_results: Dict[str, Dict[str, Any]] = Field(default_factory=dict, description="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤")',
             'evaluation_results: Annotated[Dict[str, Dict[str, Any]], "evaluation_results"] = Field(default_factory=dict, description="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤")'),
        ]
        
        for old_field, new_field in fields_to_annotate:
            if old_field in content:
                content = content.replace(old_field, new_field)
                print(f"‚úÖ –ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω: {old_field.split(':')[0]}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º reducer –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
    if "class WorkflowState(BaseModel):" in content and "reducer=" not in content:
        # –ù–∞—Ö–æ–¥–∏–º –º–µ—Å—Ç–æ –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–µ–π –∫–ª–∞—Å—Å–∞
        lines = content.split('\n')
        new_lines = []
        in_workflow_class = False
        
        for line in lines:
            new_lines.append(line)
            
            if "class WorkflowState(BaseModel):" in line:
                in_workflow_class = True
            elif in_workflow_class and line.strip().startswith('class Config:'):
                # –î–æ–±–∞–≤–ª—è–µ–º reducer –ø–µ—Ä–µ–¥ Config
                new_lines.insert(-1, '')
                new_lines.insert(-1, '    # Reducer –¥–ª—è handling concurrent updates')
                new_lines.insert(-1, '    @staticmethod')
                new_lines.insert(-1, '    def assessment_id_reducer(left: Optional[str], right: Optional[str]) -> Optional[str]:')
                new_lines.insert(-1, '        """Reducer –¥–ª—è assessment_id - –±–µ—Ä–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ"""')
                new_lines.insert(-1, '        return left if left is not None else right')
                new_lines.insert(-1, '')
                new_lines.insert(-1, '    @staticmethod')
                new_lines.insert(-1, '    def evaluation_results_reducer(left: Dict, right: Dict) -> Dict:')
                new_lines.insert(-1, '        """Reducer –¥–ª—è evaluation_results - –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""')
                new_lines.insert(-1, '        result = left.copy() if left else {}')
                new_lines.insert(-1, '        if right:')
                new_lines.insert(-1, '            result.update(right)')
                new_lines.insert(-1, '        return result')
                new_lines.insert(-1, '')
                break
        
        content = '\n'.join(new_lines)
        print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã reducers –¥–ª—è concurrent updates")
    
    risk_models_file.write_text(content, encoding='utf-8')
    return True

def fix_evaluator_json_parsing():
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ –∞–≥–µ–Ω—Ç–∞—Ö-–æ—Ü–µ–Ω—â–∏–∫–∞—Ö"""
    print("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ –∞–≥–µ–Ω—Ç–∞—Ö-–æ—Ü–µ–Ω—â–∏–∫–∞—Ö...")
    
    evaluator_file = Path("src/agents/evaluator_agents.py")
    
    if not evaluator_file.exists():
        print("‚ùå –§–∞–π–ª evaluator_agents.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    content = evaluator_file.read_text(encoding='utf-8')
    
    # –ò—â–µ–º –º–µ—Ç–æ–¥ _parse_llm_response –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ
    old_parse_method = '''    def _parse_llm_response(self, response_content: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º JSON"""
        try:
            # –ò—â–µ–º JSON –±–ª–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ
            if "```json" in response_content:
                start = response_content.find("```json") + 7
                end = response_content.find("```", start)
                if end != -1:
                    json_content = response_content[start:end].strip()
                else:
                    json_content = response_content[start:].strip()
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –ø–æ —Ñ–∏–≥—É—Ä–Ω—ã–º —Å–∫–æ–±–∫–∞–º
                start = response_content.find("{")
                end = response_content.rfind("}")
                if start != -1 and end != -1 and end > start:
                    json_content = response_content[start:end+1]
                else:
                    json_content = response_content.strip()
            
            return json.loads(json_content)
            
        except json.JSONDecodeError as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}\\n–û—Ç–≤–µ—Ç: {response_content[:200]}...")'''
    
    new_parse_method = '''    def _parse_llm_response(self, response_content: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º JSON - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        try:
            # –ò—â–µ–º JSON –±–ª–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ
            if "```json" in response_content:
                start = response_content.find("```json") + 7
                end = response_content.find("```", start)
                if end != -1:
                    json_content = response_content[start:end].strip()
                else:
                    json_content = response_content[start:].strip()
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –ø–æ —Ñ–∏–≥—É—Ä–Ω—ã–º —Å–∫–æ–±–∫–∞–º
                start = response_content.find("{")
                end = response_content.rfind("}")
                if start != -1 and end != -1 and end > start:
                    json_content = response_content[start:end+1]
                else:
                    json_content = response_content.strip()
            
            parsed_data = json.loads(json_content)
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏ –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
            required_fields = {
                "probability_score": 3,  # –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                "impact_score": 3,
                "total_score": 9,
                "risk_level": "medium",
                "probability_reasoning": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ",
                "impact_reasoning": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ",
                "identified_risks": [],
                "recommendations": [],
                "suggested_controls": [],
                "confidence_level": 0.7
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
            for field, default_value in required_fields.items():
                if field not in parsed_data:
                    parsed_data[field] = default_value
                    print(f"‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ {field}: {default_value}")
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
            if "probability_score" in parsed_data:
                try:
                    parsed_data["probability_score"] = int(parsed_data["probability_score"])
                    if not (1 <= parsed_data["probability_score"] <= 5):
                        parsed_data["probability_score"] = 3
                except (ValueError, TypeError):
                    parsed_data["probability_score"] = 3
            
            if "impact_score" in parsed_data:
                try:
                    parsed_data["impact_score"] = int(parsed_data["impact_score"])
                    if not (1 <= parsed_data["impact_score"] <= 5):
                        parsed_data["impact_score"] = 3
                except (ValueError, TypeError):
                    parsed_data["impact_score"] = 3
            
            # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º total_score
            parsed_data["total_score"] = parsed_data["probability_score"] * parsed_data["impact_score"]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º risk_level –Ω–∞ –æ—Å–Ω–æ–≤–µ total_score
            total_score = parsed_data["total_score"]
            if total_score <= 6:
                parsed_data["risk_level"] = "low"
            elif total_score <= 14:
                parsed_data["risk_level"] = "medium"
            else:
                parsed_data["risk_level"] = "high"
            
            return parsed_data
            
        except json.JSONDecodeError as e:
            # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {e}")
            return {
                "probability_score": 3,
                "impact_score": 3,
                "total_score": 9,
                "risk_level": "medium",
                "probability_reasoning": f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç LLM: {str(e)}",
                "impact_reasoning": "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                "identified_risks": ["–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ LLM"],
                "recommendations": ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞"],
                "suggested_controls": ["–£–ª—É—á—à–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤"],
                "confidence_level": 0.3
            }'''
    
    if old_parse_method in content:
        content = content.replace(old_parse_method, new_parse_method)
        print("‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ _parse_llm_response")
    else:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ _parse_llm_response –¥–ª—è –∑–∞–º–µ–Ω—ã")
    
    evaluator_file.write_text(content, encoding='utf-8')
    return True

def fix_graph_builder_concurrent_updates():
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ concurrent updates –≤ graph_builder"""
    print("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ concurrent updates –≤ graph_builder...")
    
    graph_file = Path("src/workflow/graph_builder.py")
    
    if not graph_file.exists():
        print("‚ùå –§–∞–π–ª graph_builder.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    content = graph_file.read_text(encoding='utf-8')
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–æ–≤ –æ—Ü–µ–Ω—â–∏–∫–æ–≤ - —É–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    old_evaluator_nodes = '''        # 4. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ (6 —É–∑–ª–æ–≤)
        evaluator_nodes = create_evaluator_nodes_for_langgraph_fixed(self.evaluators)
        for node_name, node_func in evaluator_nodes.items():
            workflow.add_node(node_name, log_graph_node(node_name)(node_func))'''
    
    new_evaluator_nodes = '''        # 4. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ (6 —É–∑–ª–æ–≤) - —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º concurrent updates
        evaluator_nodes = create_evaluator_nodes_for_langgraph_fixed(self.evaluators)
        for node_name, node_func in evaluator_nodes.items():
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —É–∑–µ–ª –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è concurrent updates
            def create_safe_evaluator_node(original_func):
                async def safe_node(state):
                    result = await original_func(state)
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É–∑–ª–∞
                    return {"evaluation_results": result.get("evaluation_results", {})}
                return safe_node
            
            safe_node_func = create_safe_evaluator_node(node_func)
            workflow.add_node(node_name, log_graph_node(node_name)(safe_node_func))'''
    
    if old_evaluator_nodes in content:
        content = content.replace(old_evaluator_nodes, new_evaluator_nodes)
        print("‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —É–∑–ª—ã –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è concurrent updates")
    else:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ evaluator_nodes –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
    
    graph_file.write_text(content, encoding='utf-8')
    return True

